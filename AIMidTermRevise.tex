\documentclass[a4paper, 11pt]{article}

\author{Ossama Edbali}
\title{AI mid-term revision}

\begin{document}

\maketitle

\abstract{
In this article we are going to cover the topics learned during the first term of the module ``Intro to AI''.
The major topics are:
\begin{itemize}
  \item Intelligent agents
  \item Environments
  \item Uninformed search
  \item Informed search
  \item Heuristics
  \item Adversarial search and game playing
\end{itemize}

}

\section{Intelligent agents and task environments}
We can think of an agent as an entity that takes a sequence of percepts through \textbf{sensors} from the environment and maps it to an action using \textbf{actuators} (it may affect the environment). The mapping is performed by an \textbf{agent function}.

\emph{agent = architecture + program}

A rational agent is one that tries to maximise its \textbf{performance measure} for each \textbf{percept sequence}. The performance measure evaluates any sequence of environment states.

\subsection*{Task environments}
A task environment is defined by four entities (PEAS):
\begin{itemize}
  \item Performance measure
  \item Environment
  \item Actuators
  \item Sensors
\end{itemize}

The properties of task environments (TE) are:
\begin{description}
  \item[Observability (full or partial)]
    If an agent's sensors give it a full access to the state environment then we say the TE is fully observable. Fully observable environments are really helpful
    because the agent do not need to maintain an internal state recording the percept history.
  \item[Single vs. multiagent]
    This is determined by the number of agents involved in the environment. For example in most table games we have multiagent TE in which \textbf{competitivity} and \textbf{cooperation} are very important in these cases.
  \item[Deterministic vs. stochastic]
    If the next state is completely determined by the current state then we say that the TE is deterministic; otherwise, it is stochastic.
    Stochastic means that \textbf{uncertainty} about outcomes is quantified in terms of \textbf{probabilities}; on the other hand a non-deterministic TE is one in which actions are characterized by their possible outcome and no probabilities are involved.
  \item[Episodic vs. sequential]
    If the current state is completely independent from the previous state then the TE is referred as episodic (e.g. assembly line)
  \item[Static vs. dynamic]
    If the agent's action can change the state environment then the TE is called dyanamic; otherwise it is static.
  \item[Discrete vs. continuous]
  \item[Known vs. unknown]
    This does not refer to the environment itself but to the agent's knowledge about the environment. In a known environment, the outcomes for all actions are given.
\end{description}

\subsection*{Type of agents}
There are several intelligent agents; some of them are:
\begin{itemize}
  \item Simple-reflex agents
  \item Model-based reflex agents --> handle parital observability and keep track of the percept sequence up to date (internal state)
  \item Goal-based agents
  \item Utility-based agents
  \item Learning agents
\end{itemize}

\subsubsection*{Simple-reflex agents}
Simple-reflex agents are the most basic ones: they map the \emph{current} percept to an action ignoring the entire percept history.
The agent function can be implemented as a table where entries are \textbf{if-then} or \textbf{condition-action} rules.

\subsubsection*{Model-based reflex agents}
In partially observable environments we need to implement an internal state that keeps track of the state environments (i.e. has a percepts history).
In order to update the internal state of the agent the knowledge requirements are:
\begin{itemize}
  \item Information about how the world evolves without the intervention of the agent
  \item Information about how the agent's action affect the state environment
\end{itemize}

\subsubsection*{Goal-based agents}
When the agent is faced with decisions then \textbf{goal information} is needed. Therefore the agent need a strategy to find a path (possibly optimal) to reach the goal.

\subsubsection*{Utility-based agents}
Goals are not enough to generate a high-quality behaviour in most environments. We need a measure of the quality (or optimality) of the agent's actions and their outcome; this is called a \textbf{utility}. A rational utility-based agent chooses the action that maximizes the \textbf{expected utility} of the action outcomes.

\subsubsection*{Learning agents}
In this type of agent we add the concept of learning; in a unknown environment they are efficient because they try to familiarise with it in order to make it partially or fully known.

\subsection*{State representations}
An agent can deal with different state representations:
\begin{description}
  \item[Atomic] The state is just an empty box (black box) without any internal structure (indivisible)
  \item[Factored] The state can be seen as a set of attribute-value pairs (see constraint satisfaction problems)
  \item[Structured] In each state we describe objects and their relationship to each other; structured representation is more expressive than the above ones.
\end{description}

\section{Uninformed search}
As we have said, goal-based agents look ahead considering future actions and the desirability of their outcomes. In this section we will describe \textbf{problem-solving agents} and uninformed search algorithms (no additional information other than the problem definition).

\subsection*{Steps for searching}
In uninformed search we have a simple \emph{formulate, search, execute} design process described below:
\begin{itemize}
  \item In \textbf{goal formulation} we define a goal from the current situation (state) and performance measure.
  \item \textbf{Problem formulation} is the process of considering what action to take given a goal.
  \item \textbf{Search} is the process of looking for a finite sequence of actions that leads to the goal.
  \item Once a solution has been found (i.e. a finite sequence of actions leading to the goal has beef found) the agent needs to \textbf{execute} it.
\end{itemize}

\subsection*{Problems}
A problem can be defined by 5 components:
\begin{itemize}
  \item The \textbf{initial state} or initial configuration.
  \item A description of the possible \textbf{actions}.
  \item The \textbf{transition model}: a description of the outcomes of each action.
  \item The \textbf{goal test} which determines whether a state is a goal state.
  \item A \textbf{path cost} function that assigns a numeric cost to each path.
\end{itemize}
The \textbf{state space} is formed by the initial state, actions and the transition model. We can model the state space by a \textbf{graph}.
A path is a sequence of states connected by a sequence of actions.

When searching for a solution we consider different actions (or branches); we end up by constructing a \textbf{search tree}.
The nodes correspond to states and actions to edges in the graph. From a given node we need to choose an action; we do this by \textbf{expanding} the current state. Therefore we \textbf{generate} new nodes by looking up for every legal action.
The set of nodes that are available for expansion is called the \textbf{frontier}. The way in which nodes are selected for expansion determines the \textbf{search strategy} used by a specific algorithm.

\subsection*{Measuring search algorithm performance}
We can evaluate a search algorithm performace in four ways:
\begin{itemize}
  \item Completeness: if there is a solution, can the search algorithm find it?
  \item Optimality: does the strategy solution find the optimal solution?
  \item Time complexity: how long does it take to find a solution?
  \item Space complexity: how much memory is needed to perform the given search algorithm?
\end{itemize}

\subsection*{Breadth-first search}
\textbf{BFS} is a simple search strategy that starts from the initial state and expands its children then their successors and so on (the shallowest unexpanded node is chosen for expansion). The data structure for the frontier is a FIFO queue (First In First Out). Now let's look at the performance for BFS:
\begin{itemize}
  \item Completeness: BFS is complete because it looks for all children of a given node
  \item Optimality: BFS is not optimal
  \item Time complexity: $O(b)$
\end{itemize}


\section{Informed search and heuristics}
\section{Adversarial search and game playing}

\end{document}