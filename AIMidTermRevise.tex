\documentclass[a4paper, 11pt]{article}

\usepackage{hyperref}

\author{Ossama Edbali}
\title{AI mid-term revision}

\begin{document}

\maketitle

\abstract{
In this article we are going to cover the topics learned during the first term of the module ``Intro to AI''.
The major topics are:
\begin{itemize}
  \item Intelligent agents
  \item Environments
  \item Uninformed search
  \item Informed search
  \item Heuristics
  \item Adversarial search and game playing
\end{itemize}
}

\section{Intelligent agents and task environments}
We can think of an agent as an entity that takes a sequence of percepts through \textbf{sensors} from the environment and maps it to an action using \textbf{actuators} (it may affect the environment). The mapping is performed by an \textbf{agent function}.

\emph{agent = architecture + program}

A rational agent is one that tries to maximise its \textbf{performance measure} for each \textbf{percept sequence}. The performance measure evaluates any sequence of environment states.

\subsection{Task environments}
A task environment is defined by four entities (PEAS):
\begin{itemize}
  \item Performance measure
  \item Environment
  \item Actuators
  \item Sensors
\end{itemize}

The properties of task environments (TE) are:
\begin{description}
  \item[Observability (full or partial)]
    If an agent's sensors give it a full access to the state environment then we say the TE is fully observable. Fully observable environments are really helpful
    because the agent do not need to maintain an internal state recording the percept history.
  \item[Single vs. multiagent]
    This is determined by the number of agents involved in the environment. For example in most table games we have multiagent TE in which \textbf{competitivity} and \textbf{cooperation} are very important in these cases.
  \item[Deterministic vs. stochastic]
    If the next state is completely determined by the current state then we say that the TE is deterministic; otherwise, it is stochastic.
    Stochastic means that \textbf{uncertainty} about outcomes is quantified in terms of \textbf{probabilities}; on the other hand a non-deterministic TE is one in which actions are characterized by their possible outcome and no probabilities are involved.
  \item[Episodic vs. sequential]
    If the current state is completely independent from the previous state then the TE is referred as episodic (e.g. assembly line)
  \item[Static vs. dynamic]
    If the agent's action can change the state environment then the TE is called dyanamic; otherwise it is static.
  \item[Discrete vs. continuous]
  \item[Known vs. unknown]
    This does not refer to the environment itself but to the agent's knowledge about the environment. In a known environment, the outcomes for all actions are given.
\end{description}

\subsection{Type of agents}
There are several intelligent agents; some of them are:
\begin{itemize}
  \item Simple-reflex agents
  \item Model-based reflex agents --> handle parital observability and keep track of the percept sequence up to date (internal state)
  \item Goal-based agents
  \item Utility-based agents
  \item Learning agents
\end{itemize}

\subsubsection{Simple-reflex agents}
Simple-reflex agents are the most basic ones: they map the \emph{current} percept to an action ignoring the entire percept history.
The agent function can be implemented as a table where entries are \textbf{if-then} or \textbf{condition-action} rules.

\subsubsection{Model-based reflex agents}
In partially observable environments we need to implement an internal state that keeps track of the state environments (i.e. has a percepts history).
In order to update the internal state of the agent the knowledge requirements are:
\begin{itemize}
  \item Information about how the world evolves without the intervention of the agent
  \item Information about how the agent's action affect the state environment
\end{itemize}

\subsubsection{Goal-based agents}
When the agent is faced with decisions then \textbf{goal information} is needed. Therefore the agent need a strategy to find a path (possibly optimal) to reach the goal.

\subsubsection{Utility-based agents}
Goals are not enough to generate a high-quality behaviour in most environments. We need a measure of the quality (or optimality) of the agent's actions and their outcome; this is called a \textbf{utility}. A rational utility-based agent chooses the action that maximizes the \textbf{expected utility} of the action outcomes.

\subsubsection{Learning agents}
In this type of agent we add the concept of learning; in a unknown environment they are efficient because they try to familiarise with it in order to make it partially or fully known.

\subsection{State representations}
An agent can deal with different state representations:
\begin{description}
  \item[Atomic] The state is just an empty box (black box) without any internal structure (indivisible)
  \item[Factored] The state can be seen as a set of attribute-value pairs (see constraint satisfaction problems)
  \item[Structured] In each state we describe objects and their relationship to each other; structured representation is more expressive than the above ones.
\end{description}

\section{Uninformed search}
As we have said, goal-based agents look ahead considering future actions and the desirability of their outcomes. In this section we will describe \textbf{problem-solving agents} and uninformed search algorithms (no additional information other than the problem definition).

\subsection{Steps for searching}
In uninformed search we have a simple \emph{formulate, search, execute} design process described below:
\begin{itemize}
  \item In \textbf{goal formulation} we define a goal from the current situation (state) and performance measure.
  \item \textbf{Problem formulation} is the process of considering what action to take given a goal.
  \item \textbf{Search} is the process of looking for a finite sequence of actions that leads to the goal.
  \item Once a solution has been found (i.e. a finite sequence of actions leading to the goal has beef found) the agent needs to \textbf{execute} it.
\end{itemize}

\subsection{Problems}
A problem can be defined by 5 components:
\begin{itemize}
  \item The \textbf{initial state} or initial configuration.
  \item A description of the possible \textbf{actions}.
  \item The \textbf{transition model}: a description of the outcomes of each action.
  \item The \textbf{goal test} which determines whether a state is a goal state.
  \item A \textbf{path cost} function that assigns a numeric cost to each path.
\end{itemize}
The \textbf{state space} is formed by the initial state, actions and the transition model. We can model the state space by a \textbf{graph}.
A path is a sequence of states connected by a sequence of actions.

When searching for a solution we consider different actions (or branches); we end up by constructing a \textbf{search tree}.
The nodes correspond to states and actions to edges in the graph. From a given node we need to choose an action; we do this by \textbf{expanding} the current state. Therefore we \textbf{generate} new nodes by looking up for every legal action.
The set of nodes that are available for expansion is called the \textbf{frontier}. The way in which nodes are selected for expansion determines the \textbf{search strategy} used by a specific algorithm.

\subsection{Measuring search algorithm performance}
We can evaluate a search algorithm performace in four ways:
\begin{itemize}
  \item Completeness: if there is a solution, can the search algorithm find it?
  \item Optimality: does the strategy solution find the optimal solution?
  \item Time complexity: how long does it take to find a solution?
  \item Space complexity: how much memory is needed to perform the given search algorithm?
\end{itemize}

\subsection{Breadth-first search}
\textbf{BFS} is a simple search strategy that starts from the initial state and expands its children then their successors and so on (the shallowest unexpanded node is chosen for expansion). The data structure for the frontier is a FIFO queue (First In First Out). Now let's look at the performance for BFS:
\begin{itemize}
  \item Completeness: BFS is complete because it looks for all children of a given node (for finite branching factor)
  \item Optimality: BFS is not optimal
  \item Time complexity: $O(b^d)$ where $b$ is the \textbf{branching factor} and $d$ is the \textbf{depth} of the solution
  \item Space complexity: $O(b^d)$
\end{itemize}

\subsection{Uniform cost search}
BFS is optimal if the step costs are equal. When dealing with different step costs we must chose the node with the lowest path cost $g(n)$ (not the shallowest node). This is achieved by having the frontier as a \textbf{priority queue}. Another difference with BFS is that the goal test is applied when a node is selected for expansion and not when it is first generated.

Uniform cost search is generally optimal and completeness is guaranteed provided that the cost of every step exceeds some small positive constant $\varepsilon$.
When all step costs are the same, uniform cost search acts like BFS. Time and space complexity do not simply depend on $b$ and $d$ because there are involved path costs (not depths). Let $C*$ be the cost of the optimal solution, and assume that every action costs at least $\varepsilon$. Therefore, in the worst case time and space complexity is $O(b^{1+ [C*/\varepsilon]})$.

\subsection{Depth-first search}
\textbf{DFS} always expands the deepest node in the current frontier of the search tree. When DFS reaches a leaf node it \emph{backs up} to the next deepest node that has still unexplored children. The frontier is implemented as a LIFO stack (Last In First Out) because it picks up the newest generated node.
The tree-search version of DFS is not complete in infinite state space (the graph-search version is complete in finite space) nor optimal.

Let $m$ be the maximum depth of any node, thus the time complexity is $O(b^m)$ and the space complexity is $O(b*m)$ because we keep track only of the nodes on the current path.

\subsection{Depth-limited search}
For infinite state spaces DFS is incomplete; a solution may be setting a limit to the search $l$. Thus level at depth $l$ are treated as leaf nodes. Time complexity is $O(b^l)$ and space complexity is $O(b*l)$.

\subsection{Iterative deepening depth-first search}
The basic idea of iterative deepening search is:
\begin{itemize}
  \item Explore to one level
  \item If no solution is found, restart from the root and explore to the next level
\end{itemize}

IDS is complete even for infinite state spaces, is optimal and its space complexity is $O(b*d)$ because it keeps only the recent path. As for time complexity it may seem that IDS do a lot of work but only the current iteration is relevant (the previous ones are asymptotically insignificant). Thus, time complexity for IDS is $O(b^d)$.

\section{Informed search}
Informed search strategies uses problem-specific knowledge in addition to the information given by the problem definition.
The general approach is called \textbf{best-first search} in which we use an \textbf{evaluation function} $f(n)$ to expand nodes. The node with the lowest $f(n)$ will be selected first for expansion. The choice of $f$ determines the search strategy. Usually $f$ has a \textbf{heuristic} component $h$:

\indent $h(n)$: the estimated cost of the cheapest path from the state at node $n$ to the goal state.

\subsection{Greedy best-first search}
\textbf{Greedy best-first search} tries to expand the nodes that are near to the goal state. That is it expands the nodes that have the lowest $f(n) = h(n)$.
GBFS is not optimal and not complete even for finite state space. It is incomplete because it could reach dead ends.

\subsection{A* search}
\textbf{A* search} uses both the heuristic function and the cost function (cost to reach the node).

\indent $f(n) = g(n) + h(n)$

A* search is both complete and optimal (if the heuristic function satisfies some conditions); it is like uniform cost search but it uses $f$ and $g$ instead of only $g$.

\subsubsection{Conditions for optimality: admissibility and consistency}
\begin{description}
  \item[Admissibility] A heuristic function is admissible if it \emph{never overstimate} the cost to reach the goal.
  \item[Consistency] $h(n) \le c(n, a, n') + h(n')$ where $c(n, a, n')$ is the cost of getting from node $n$ to node $n'$ with some action $a$
\end{description}

\section{Heuristics}
Choosing a good heuristic function is very important for the performance of an algorithm. One way to determine the quality is the \textbf{effective branching factor} $b*$.
It is defined as:

\[ N + 1 = 1 + b* + (b*)^2 + (b*)^3 + \ldots + (b*)^d \]

Let us consider two heuristic functions $h_1$ and $h_2$ such that for every $n$ we have $h_2(n) \ge h_1(n)$. In this case we say that $h_2$ \textbf{dominates} $h_1$.

\subsection{Ways to generate heuristic functions}
We can generate heuristic functions from:
\begin{description}
  \item[Relaxed problems] When we relax the constraints of a problem we get a subproblem which is generally simpler to solve. The state space grows and if there is an optimal solution in the original problem then there will be also in the relaxed problem. So, \emph{the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem}.
  \item[Pattern databases]
  \item[Experience]
\end{description}

\section{Beyond classical search}

\subsection{Background}
In \textbf{local search} (in the state space) algorithms we deal with relaxed concepts of observability and determinism. Local search algorithms evaluate and modify one current state rather than systematically exploring paths from the initial state. These algorithms are suitable when we are just interested in the goal rather than the cost to get to a final state. If the path to the goal does not matter, we might consider a different class of algorithms that do not care about paths at all.
Local search algorithms operate on a \textbf{current node} and generally move only to neighbours.
Key advantages of local search:
\begin{itemize}
  \item Little usage of memory
  \item Find solutions in reasonable times even for infinite state spaces
\end{itemize}

Local search algorithms are also useful for solving \textbf{optimisation problems} in which the aim is to find the best state according to an \textbf{objective function}.
We may consider a state-space landscape where the elevation corresponds to:
\begin{itemize}
  \item Cost function: \textbf{global minimum}
  \item Objective function: \textbf{global maximum}
\end{itemize}

\subsection{Hill climbing}
The basic idea of hill climbing search (\textbf{steepest-ascent} version) tries to improve the score of the objective function until it reaches a ``peak'' when no higher score states are reachable.
This algorithm is also called \textbf{greedy local search} because it grabs a good neighbour state without looking ahead.

Hill climbing search can get stuck on:
\begin{itemize}
  \item Local maxima (or minima)
  \item Ridges: sequence of local maxima (or minima)
  \item Plateaux: shoulder (from which progress is possible) or flat maximum (no uphill exists)
\end{itemize}

The possible solutions to the above issues are:
\begin{itemize}
  \item Randomise and restart
  \item Increase the size of visitable neighbours (look ahead)
  \item Stochastic hill-climbing
\end{itemize}

\subsection{Simulated annealing}
To improve the efficiency of local search we must have a \textbf{trade-off} between:
\begin{itemize}
  \item Hill-climbing search
  \item Random walk
\end{itemize}

SA derives from the process of annealing: if you heat a solid and then gradually cool it (slowly), large crystals will be formed.
In 1982 Kirkpatrick took the idea and applied it to optimisation problems.

The basic idea behind SA is: use SA to search for feasible solutions and converge to an optimal solution.
Unlike hill climbing, simulated annealing chooses a random move from the neighbourhood.

\begin{itemize}
  \item Changes the parameter $T$ (used in checking the next neighbours) during the search
  \item Starts with high values of \( T \) (i.e. random search)
  \item The value of $T$ gradually decreases (like temperature going down to zero - frozen state)
  \item To the end $T$ is very small and SA acts like pure hill climbing search
\end{itemize}

\subsection{Beam search}
\textbf{Beam search} instead of moving just with one node (keep just one state in memory) it keeps track of $k$ nodes. It begins with $k$ randomly generated states. At each step all successors of the $k$ nodes are generated. If any one of them is a goal state then the algorithm stops. \emph{In a local beam search, useful information is exchanged betweek the parallel search threads}.
In its simplest form, local beam search can suffer from a lack of diversity among the
k states.

A variant called stochastic beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
of choosing the best $k$ from the the pool of candidate successors, stochastic beam search
chooses $k$ successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a “state” (organism)
populate the next generation according to its “value” (fitness).

\subsection{Genetic algorithms}
A \textbf{genetic algorithm} (GA) is a variant of stochastic beam search were successors are generated by two \emph{parent} states rather than by modifying a single state (for further details see textbook).

\section{Adversarial search and game playing}
Adversarial search arises when we are trying to reach a goal in \textbf{multiagent environments}. In AI games are of a specialised kind called \textbf{zero-sum games} or deterministic, turn taking games.
In adversarial search \textbf{pruning} (ignore portions of the search tree) and \textbf{evaluation function} (approximate the true utility of a state without doing a completed search) are very important concepts.

A game can be defined by the following elements:
\begin{itemize}
  \item Initial state $S_0$
  \item PLAYER($s$): defines which players has the move in the state $s$
  \item ACTIONS($s$): defines the legal moves available from the state $s$
  \item RESULT($s, a$): \textbf{transition model}
  \item TERMINAL-TEST($s$): checks whether the state $s$ is a terminal node where the game ends (victory, loss, draw)
  \item UTILITY($s, p$): A \textbf{utility function} defines the final numeric value for a game that ends in the state $s$ for a player $p$.
\end{itemize}

\subsection{Minimax algorithm}
Given a game tree, can be determined from the \textbf{minimax} value of each node, MINIMAX($n$).
The minimax value of a node is the utility (for MAX) of being in the corresponding state, assuming that both players play optimally
from there to the end of the game. Obviously, the minimax value of a terminal state is just
its utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value,
whereas MIN prefers a state of minimum value.

The \textbf{minmax algorithm} computes the minimax decision from the current state. It uses a recursive computation of minimax values of each successor state. The recursion proceeds all the way down to the leaves where the minimax values are \textbf{backed up} through the tree.

The minimax algorithm performs a complete depth-first search of the game tree. Let $m$ be the maximum depth and $b$ the possible legal moves at each stage, the time complexity is $O(b^m)$ and the space complexity is $O(bm)$.

The pseudo-code is as follows:
\begin{verbatim}
function minimax(node, depth, maxPlayer)
    if depth = 0 or node is a terminal node
        return the heuristic value of node
    if maximizingPlayer
        bestValue := -inf
        for each child of node
            val := minimax(child, depth - 1, FALSE)
            bestValue := max(bestValue, val)
        return bestValue
    else
        bestValue := +inf
        for each child of node
            val := minimax(child, depth - 1, TRUE)
            bestValue := min(bestValue, val)
        return bestValue
\end{verbatim}

\subsection{Alpha-beta pruning}
The problem with minimax algorithm is that the nodes generated are exponential in the depth of the tree.
We can use the concept of \textbf{pruning}, that is eliminate from the search tree paths that are likely to be useless for the final decision (not influencial).

Visual explanation: \url{www.youtube.com/watch?v=xBXHtz4Gbdo}

\end{document}